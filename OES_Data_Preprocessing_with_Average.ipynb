{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 라이브러리 선언\n",
    "\n",
    "\n",
    "## 표준 라이브러리\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "## 서드파티 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cacb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Range\n",
    "\n",
    "\n",
    "## Total Flow Rates\n",
    "total_flow_rates = [80, 90, 100, 110, 120, 130, 140]\n",
    "\n",
    "\n",
    "## Equivalence Ratios\n",
    "equivalence_ratios = [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "\n",
    "\n",
    "## Mix Ratios\n",
    "mix_ratios = [0, 3.75, 7.5, 11.25, 15, 18.75, 22.5, 26.25, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0396f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Declaration of Variables\n",
    "\n",
    "\n",
    "## Number of OES data for each Experiment Conditions\n",
    "NUM_OF_SPECTRUM = 500\n",
    "\n",
    "## Number of OES data after reflecting the Average\n",
    "NUM_OF_AVG_SPECTRUM = 100\n",
    "\n",
    "## Declaration of Distribution Variable\n",
    "NUM_OF_CONDITION = 0\n",
    "\n",
    "\n",
    "## Ratio of Validation Dataset\n",
    "VALID_DATA_RATIO = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Data from Excel File\n",
    "\n",
    "\n",
    "## File path of Spectrum Data\n",
    "spectrum_file_path = \"/Users/RFL/SeoyeonYoun/PT/OES_data\"\n",
    "\n",
    "\n",
    "## File path of Actual Experimental Conditon Data\n",
    "actual_condition_df = pd.read_excel(\"/Users/RFL/SeoyeonYoun/PT/ActualCondition_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aff8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input and Output Dataset List (Train, Test, and Valid)\n",
    "\n",
    "\n",
    "## Input X = Spectrum Data\n",
    "X_Train = []\n",
    "X_Test = []\n",
    "X_Valid = []\n",
    "\n",
    "\n",
    "## Output Y = TotalFLowRate, EquivalenceRatio, MixRatio\n",
    "Y_TFR_Train = []\n",
    "Y_TFR_Test = []\n",
    "Y_TFR_Valid = []\n",
    "\n",
    "Y_ER_Train = []\n",
    "Y_ER_Test = []\n",
    "Y_ER_Valid = []\n",
    "\n",
    "Y_MR_Train = []\n",
    "Y_MR_Test = []\n",
    "Y_MR_Valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec4323",
   "metadata": {},
   "outputs": [],
   "source": [
    "### File Directory Function and File Name Function\n",
    "\n",
    "\n",
    "## File Directory Function\n",
    "def find_directory(forder_path, v_tfr, v_er, v_mr ):\n",
    "    \n",
    "    # File Directory 저장하기\n",
    "    file_directory = os.path.join(forder_path, str(v_tfr), str(v_er), str(v_mr))\n",
    "\n",
    "    # Directory가 실제로 존재하는지 확인\n",
    "    if not os.path.exists(file_directory):\n",
    "        raise FileNotFoundError(f'Directory not found: {file_directory}')\n",
    "    \n",
    "    return file_directory\n",
    "\n",
    "\n",
    "## File Name Function\n",
    "def find_name(file_directory):\n",
    "    \n",
    "    # .xlsx 파일 목록 필터링\n",
    "    file_list = os.listdir(file_directory)\n",
    "\n",
    "    # .xlsx 파일이 실제로 존재하는지 확인인\n",
    "    if not file_list:\n",
    "        raise FileNotFoundError(f'No .xlsx files found in {file_directory}')\n",
    "    \n",
    "    # File Name 생성\n",
    "    file_name = os.path.join(file_directory, file_list[0])\n",
    "\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d28d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Preprocessing Function\n",
    "\n",
    "def preprocess_data(name_of_file):\n",
    "    \n",
    "    try:\n",
    "        # Excel file을 DataFrame 형태로로 읽어오고, 원하는 범위로 자르기\n",
    "        spectrum_data = pd.read_excel(name_of_file, header=None)\n",
    "        spectrum_data = spectrum_data.iloc[5:1605, 1:NUM_OF_SPECTRUM+1]\n",
    "\n",
    "        # 데이터 타입을 float으로로 변환 (for Interpolation)\n",
    "        spectrum_data = spectrum_data.astype(float)\n",
    "                    \n",
    "        # 음수 값 또는 20000 초과 값을 NaN으로 변경\n",
    "        spectrum_data[spectrum_data<0] = np.nan\n",
    "        spectrum_data[spectrum_data>20000] = np.nan\n",
    "\n",
    "        # NaN을 Interpolate\n",
    "        spectrum_data = spectrum_data.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "        # 여전히 NaN 값이 남아있는 경우, 앞뒤 값으로 채움\n",
    "        spectrum_data = spectrum_data.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "        # 동일 조건에 대한 500개의 스펙트럼을 50개씩 묶어서 평균 내기 (1600, 500)이 (1600, 100)으로 축소\n",
    "        spectrum_data_avg = spectrum_data.groupby(np.arange(spectrum_data.shape[1])//5, axis=1).mean()\n",
    "\n",
    "        # DataFrame 형태를 NumPy 배열(ndarray)             \n",
    "        spectrum_data_avg = spectrum_data_avg.values\n",
    "\n",
    "        return spectrum_data_avg\n",
    "\n",
    "    except Exception as e:\n",
    "        # 앞에서 발생한 모든 예외(Exception)을 e로 받아서 출력\n",
    "        return {f'Error reading {name_of_file}: {e}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Distribution Function\n",
    "\n",
    "def distribute_data(x, y_tfr, y_er, y_mr, spectrum_data):\n",
    "    \n",
    "    # Preprocessing된 Spectrum Data를 Dataset에 분배\n",
    "    for index in range(NUM_OF_AVG_SPECTRUM):\n",
    "        x.append(spectrum_data[:, index])\n",
    "        y_tfr.append(actual_condition_df.iloc[NUM_OF_CONDITION, 0])\n",
    "        y_er.append(actual_condition_df.iloc[NUM_OF_CONDITION, 1])\n",
    "        y_mr.append(actual_condition_df.iloc[NUM_OF_CONDITION, 2])\n",
    "    \n",
    "    return x, y_tfr, y_er, y_mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135406ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Preprocessing and Distribution\n",
    "\n",
    "for index_TFR, value_TFR in enumerate(total_flow_rates, start=0):\n",
    "    for index_ER, value_ER in enumerate(equivalence_ratios, start=0):\n",
    "        for index_MR, value_MR in enumerate(mix_ratios, start=0):\n",
    "            \n",
    "            # Progress 판단을 위한 임시 출력\n",
    "            # print([value_TFR, value_ER, value_MR])\n",
    "\n",
    "            # File Directory 생성\n",
    "            file_directory = find_directory(spectrum_file_path, value_TFR, value_ER, value_MR)\n",
    "\n",
    "            # File Name 생성\n",
    "            file_name = find_name(file_directory)\n",
    "\n",
    "            # Spectrum Data Preprocessing 진행\n",
    "            spectrum_data_avg = preprocess_data(file_name)\n",
    "\n",
    "            # Validation Dataset의 비율 조정을 위한 변수\n",
    "            VALID_SCORE = random.random()\n",
    "\n",
    "            # Spectrum Daata Distribution 진행\n",
    "            if (index_TFR+index_ER+index_MR) % 2 == 0:\n",
    "                X_Train, Y_TFR_Train, Y_ER_Train, Y_MR_Train = distribute_data(X_Train, Y_TFR_Train, Y_ER_Train, Y_MR_Train, spectrum_data_avg)\n",
    "                print([value_TFR, value_ER, value_MR])\n",
    "            elif (index_TFR+index_ER+index_MR) % 2 == 1:\n",
    "                if (VALID_DATA_RATIO >= VALID_SCORE):\n",
    "                    X_Valid, Y_TFR_Valid, Y_ER_Valid, Y_MR_Valid = distribute_data(X_Valid, Y_TFR_Valid, Y_ER_Valid, Y_MR_Valid, spectrum_data_avg)\n",
    "                else:\n",
    "                    X_Test, Y_TFR_Test, Y_ER_Test, Y_MR_Test = distribute_data(X_Test, Y_TFR_Test, Y_ER_Test, Y_MR_Test, spectrum_data_avg)\n",
    "            else:\n",
    "                print(\"Data distribution error!\\n\")\n",
    "                print(f'Error @ {index_TFR}, {index_ER}, {index_MR}\\n')\n",
    "                print(f'Error @ {value_TFR}, {value_ER}, {value_MR}\\n')\n",
    "\n",
    "            # Global Variable 업데이트\n",
    "            NUM_OF_CONDITION += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check Results of Data Preprocessing and Distribution - (1)\n",
    "\n",
    "\n",
    "## Train Dataset for Spectrum Data \n",
    "print(\"X_Train Data의 경우\")\n",
    "print(max(len(x_Train) for x_Train in X_Train))\n",
    "print(min(len(x_Train) for x_Train in X_Train))\n",
    "\n",
    "\n",
    "## Test Dataset for Spectrum Data \n",
    "print(\"X_Test Data의 경우\")\n",
    "print(max(len(x_Test) for x_Test in X_Test))\n",
    "print(min(len(x_Test) for x_Test in X_Test))\n",
    "\n",
    "\n",
    "## Valid Dataset for Spectrum Data \n",
    "print(\"X_Valid Data의 경우\")\n",
    "print(max(len(x_Valid) for x_Valid in X_Valid))\n",
    "print(min(len(x_Valid) for x_Valid in X_Valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf069df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check Results of Data Preprocessing and Distribution - (2)\n",
    "\n",
    "\n",
    "## Function for Making Listed List Flat \n",
    "def flatten_list(nested_list):\n",
    "    \n",
    "    flat_list = [item for sublist in nested_list for item in sublist]\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "## Train Spectrum Dataset의 최대값과 최소값 구하기기\n",
    "print(\"X_Train Data의 경우\")\n",
    "print(f'최대값: {max(flatten_list(X_Train))}')\n",
    "print(f'최소값: {min(flatten_list(X_Train))}')\n",
    "\n",
    "\n",
    "## Test Spectrum Dataset의 최대값과 최소값 구하기\n",
    "print(\"X_Test Data의 경우\")\n",
    "print(f'최대값: {max(flatten_list(X_Test))}')\n",
    "print(f'최소값: {min(flatten_list(X_Test))}')\n",
    "\n",
    "\n",
    "## Valid Spectrum Dataset의 최대값과 최소값 구하기\n",
    "print(\"X_Valid Data의 경우\")\n",
    "print(f'최대값: {max(flatten_list(X_Valid))}')\n",
    "print(f'최소값: {min(flatten_list(X_Valid))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check Results of Data Preprocessing and Distribution - (3)\n",
    "\n",
    "\n",
    "## Input X의 길이 확인\n",
    "print(f'X_Train Data Condition = {int(len(X_Train)/100)}')\n",
    "print(f'X_Test Data Condition = {int(len(X_Test)/100)}')\n",
    "print(f'X_Valid Data Condition = {int(len(X_Valid)/100)}')\n",
    "\n",
    "\n",
    "## Output Y_TFR의 길이 확인\n",
    "print(f'Y_TFR_Train Data Condition = {int(len(Y_TFR_Train)/100)}')\n",
    "print(f'Y_TFR_Test Data Condition = {int(len(Y_TFR_Test)/100)}')\n",
    "print(f'Y_TFR_Valid Data Condition = {int(len(Y_TFR_Valid)/100)}')\n",
    "\n",
    "\n",
    "## Output Y_ER의 길이 확인\n",
    "print(f'Y_ER_Train Data Condition = {int(len(Y_ER_Train)/100)}')\n",
    "print(f'Y_ER_Test Data Condition = {int(len(Y_ER_Test)/100)}')\n",
    "print(f'Y_ER_Valid Data Condition = {int(len(Y_ER_Valid)/100)}')\n",
    "\n",
    "\n",
    "## Output Y_MR의 길이 확인\n",
    "print(f'Y_MR_Train Data Condition = {int(len(Y_MR_Train)/100)}')\n",
    "print(f'Y_MR_Test Data Condition = {int(len(Y_MR_Test)/100)}')\n",
    "print(f'Y_MR_Valid Data Condition = {int(len(Y_MR_Valid)/100)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7419405",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change into NumPy Array\n",
    "\n",
    "\n",
    "## Input X\n",
    "X_Train = np.array(X_Train)\n",
    "X_Test = np.array(X_Test)\n",
    "X_Valid = np.array(X_Valid)\n",
    "\n",
    "\n",
    "## Output Y_TFR\n",
    "Y_TFR_Train = np.array(Y_TFR_Train)\n",
    "Y_TFR_Test = np.array(Y_TFR_Test)\n",
    "Y_TFR_Valid = np.array(Y_TFR_Valid)\n",
    "\n",
    "\n",
    "## Output Y_ER\n",
    "Y_ER_Train = np.array(Y_ER_Train)\n",
    "Y_ER_Test = np.array(Y_ER_Test)\n",
    "Y_ER_Valid = np.array(Y_ER_Valid)\n",
    "\n",
    "\n",
    "## Output Y_MR\n",
    "Y_MR_Train = np.array(Y_MR_Train)\n",
    "Y_MR_Test = np.array(Y_MR_Test)\n",
    "Y_MR_Valid = np.array(Y_MR_Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf901b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check Results of NumPy Array - (4)\n",
    "\n",
    "\n",
    "## Input X\n",
    "print(f'X_Train의 크기: {X_Train.shape}')\n",
    "print(f'X_Test의 크기: {X_Test.shape}')\n",
    "print(f'X_Valid의 크기: {X_Valid.shape}')\n",
    "\n",
    "\n",
    "## Output Y_TFR\n",
    "print(f'Y_TFR_Train의 크기: {Y_TFR_Train.shape}')\n",
    "print(f'Y_TFR_Test의 크기: {Y_TFR_Test.shape}')\n",
    "print(f'Y_TFR_Valid의 크기: {Y_TFR_Valid.shape}')\n",
    "\n",
    "\n",
    "## Output Y_ER\n",
    "print(f'Y_ER_Train의 크기: {Y_TFR_Train.shape}')\n",
    "print(f'Y_ER_Test의 크기: {Y_TFR_Test.shape}')\n",
    "print(f'Y_ER_Valid의 크기: {Y_TFR_Valid.shape}')\n",
    "\n",
    "\n",
    "## Output Y_MR\n",
    "print(f'Y_MR_Train의 크기: {Y_MR_Train.shape}')\n",
    "print(f'Y_MR_Test의 크기: {Y_MR_Test.shape}')\n",
    "print(f'Y_MR_Valid의 크기: {Y_MR_Valid.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e38cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalization Function of Spectrum Data\n",
    "\n",
    "def normalize_x(x_target, x_train, x_test, x_valid):\n",
    "    \n",
    "    # Spectrum 데이터 값 중 최대값 찾기\n",
    "    max_x_spectrum = np.max([np.max(x_train), np.max(x_test), np.max(x_valid)])\n",
    "\n",
    "    # Spectrum 데이터 값 중 최대값 출력 및 확인\n",
    "    print(f'Normalization을 위한 Spectrum 데이터 중 최대값: {max_x_spectrum}')\n",
    "\n",
    "    # Normalized 완료된 Spectrum Data를 저장할 새로운 Array 변수 생성\n",
    "    normalized_x_target = np.zeros(len(x_target)*1600).reshape(len(x_target), 1600)\n",
    "\n",
    "    # Normalization 진행\n",
    "    for index in range(len(x_target)):\n",
    "        normalized_x_target[index, :] = x_target[index, :] / max_x_spectrum\n",
    "\n",
    "    return normalized_x_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalization of Input X\n",
    "\n",
    "\n",
    "## Normalization of X_Train\n",
    "normalized_X_Train = normalize_x(X_Train, X_Train, X_Test, X_Valid)\n",
    "\n",
    "\n",
    "## Normalization of X_Test\n",
    "normalized_X_Test = normalize_x(X_Test, X_Train, X_Test, X_Valid)\n",
    "\n",
    "\n",
    "## Normalization of X_Valid\n",
    "normalized_X_Valid = normalize_x(X_Valid, X_Train, X_Test, X_Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7634195",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalization Function of Actual Condition\n",
    "\n",
    "def normalize_y(y_target_train, y_target_test, y_target_valid, actual_condition_dataframe, column_index):\n",
    "    \n",
    "    # Actual Condition에서 Target 조건의 최소값과 최대값 찾기\n",
    "    min_y_target = actual_condition_dataframe.iloc[:, column_index].min()\n",
    "    max_y_target = actual_condition_dataframe.iloc[:, column_index].max()\n",
    "\n",
    "    # Normalization 진행\n",
    "    y_target_train = (y_target_train - min_y_target) / (max_y_target - min_y_target)\n",
    "    y_target_test = (y_target_test - min_y_target) / (max_y_target - min_y_target)\n",
    "    y_target_valid = (y_target_valid - min_y_target) / (max_y_target - min_y_target)\n",
    "\n",
    "    return y_target_train, y_target_test, y_target_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2af520",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalization of Output Y\n",
    "\n",
    "\n",
    "## Normalization of Y_TFR\n",
    "Y_TFR_Train, Y_TFR_Test, Y_TFR_Valid = normalize_y(Y_TFR_Train, Y_TFR_Test, Y_TFR_Valid, actual_condition_df, 0)\n",
    "\n",
    "\n",
    "## Normalization of Y_ER\n",
    "Y_ER_Train, Y_ER_Test, Y_ER_Valid = normalize_y(Y_ER_Train, Y_ER_Test, Y_ER_Valid, actual_condition_df, 1)\n",
    "\n",
    "\n",
    "## Normalization of Y_MR\n",
    "Y_MR_Train, Y_MR_Test, Y_MR_Valid = normalize_y(Y_MR_Train, Y_MR_Test, Y_MR_Valid, actual_condition_df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NumPy 파일로 변수 저장\n",
    "\"\"\"\n",
    "np.save(\"Normalized_X_Train.npy\", normalized_X_Train)\n",
    "np.save(\"Normalized_X_Test.npy\", normalized_X_Test)\n",
    "np.save(\"Normalized_X_Valid.npy\", normalized_X_Valid)\n",
    "\n",
    "np.save(\"Y_TFR_Train.npy\", Y_TFR_Train)\n",
    "np.save(\"Y_TFR_Test.npy\", Y_TFR_Test)\n",
    "np.save(\"Y_TFR_Valid.npy\", Y_TFR_Valid)\n",
    "\n",
    "np.save(\"Y_ER_Train.npy\", Y_ER_Train)\n",
    "np.save(\"Y_ER_Test.npy\", Y_ER_Test)\n",
    "np.save(\"Y_ER_Valid.npy\", Y_ER_Valid)\n",
    "\n",
    "np.save(\"Y_MR_Train.npy\", Y_MR_Train)\n",
    "np.save(\"Y_MR_Test.npy\", Y_MR_Test)\n",
    "np.save(\"Y_MR_Valid.npy\", Y_MR_Valid)\n",
    "\n",
    "print(\"NumPy 배열 변수들 저장 완료!\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT_seoyeon1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
